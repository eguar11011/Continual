##  Descripci贸n general del proyecto

-   **Nombre del c贸digo:** trainer.py
-   **Versi贸n:** N/A
-   **Explicaci贸n general:** Este script implementa un orquestrador de entrenamiento para aprendizaje continuo (continual learning), con funcionalidades de guardado por tarea y m茅tricas completas. Permite entrenar un modelo en una secuencia de tareas, guardando checkpoints, predicciones y etiquetas para cada tarea, calculando matrices de confusi贸n y registrando m茅tricas de rendimiento.
-   **Qu茅 problema resuelve el c贸digo:** Aborda el problema del olvido catastr贸fico en modelos de aprendizaje autom谩tico al entrenarlos secuencialmente en diferentes tareas. Proporciona un marco para evaluar y mitigar este problema mediante el uso de diferentes estrategias de aprendizaje continuo.

## 锔 Visi贸n general del sistema

```mermaid
graph LR
    A[Datasets] --> B(DataLoader)
    B --> C{Trainer}
    C --> D[Learner]
    D --> E(Model)
    E --> F[Loss Function]
    F --> D
    C --> G{Metrics}
    G --> H[Output Files]
```

-   **Tecnolog铆as utilizadas:**
    *   Python 3
    *   PyTorch
    *   tqdm
    *   PyYAML
    *   JSON
    *   CSV
-   **Dependencias:**
    *   torch
    *   torch.utils.data
    *   tqdm
    *   PyYAML
    *   argparse
    *   pathlib
    *   typing
    *   csv
    *   json
-   **Requisitos del sistema:**
    *   Entorno Python 3.
    *   Instalaci贸n de las dependencias listadas.
    *   Opcional: GPU compatible con CUDA para aceleraci贸n del entrenamiento.
-   **Prerrequisitos:**
    *   Conocimiento b谩sico de PyTorch y aprendizaje autom谩tico.
    *   Familiaridad con conceptos de aprendizaje continuo (continual learning).

##  Gu铆a de uso

-   **C贸mo usarlo:** El script se ejecuta desde la l铆nea de comandos, aceptando argumentos para configurar el entrenamiento, el conjunto de datos, el modelo y la estrategia de aprendizaje continuo.
-   **Explicaci贸n de los pasos:**
    1.  **Entrada:** El script recibe argumentos de l铆nea de comandos que especifican la configuraci贸n del experimento, incluyendo el conjunto de datos, la arquitectura del modelo, la estrategia de aprendizaje continuo, los hiperpar谩metros de entrenamiento y la ubicaci贸n para guardar los resultados.
    2.  **Procesamiento:**
        *   Carga y divide el conjunto de datos en tareas.
        *   Inicializa el modelo y el "learner" (implementaci贸n de la estrategia de aprendizaje continuo).
        *   Entrena el modelo secuencialmente en cada tarea.
        *   Eval煤a el modelo despu茅s de cada tarea y calcula las m茅tricas de rendimiento.
        *   Guarda los checkpoints del modelo, las predicciones, las etiquetas y las matrices de confusi贸n para cada tarea.
        *   Registra todas las m茅tricas en un archivo JSON.
    3.  **Salida:** El script genera varios archivos de salida, incluyendo:
        *   Checkpoints del modelo para cada tarea (`ckpt_t{t}.pt`).
        *   Predicciones y etiquetas para cada tarea (`preds_task{t}.pt`).
        *   Matrices de confusi贸n en formato CSV para cada tarea (`confmat_task{t}.csv`).
        *   Un archivo JSON que contiene un resumen de las m茅tricas de rendimiento (`metrics.json`).
-   **Caso de uso de ejemplo:**

```python
import torch
from models import Classifier, get_backbone
from learner import build_learner

# Define el n煤mero de clases por tarea
num_classes = 2

# Inicializa el modelo
model = Classifier(get_backbone("resnet18"), num_classes=num_classes)

# Construye un "learner" con la estrategia "finetune"
learner = build_learner("finetune", model, lr=1e-3)

# Crea datos de ejemplo (simulando un batch)
x = torch.randn(64, 3, 224, 224)  # 64 im谩genes de tama帽o 224x224
y = torch.randint(0, num_classes, (64,))  # Etiquetas para las 64 im谩genes
batch = (x, y)

# Realiza un paso de entrenamiento
loss = learner.observe(batch)

print(f"Loss: {loss:.4f}")
```

##  Documentaci贸n de la API

El c贸digo proporcionado no define una API en el sentido tradicional (endpoints HTTP, etc.). Sin embargo, define una clase `Trainer` con un m茅todo `run` que orquesta el proceso de entrenamiento.

##  Referencias

*   **PyTorch:** [https://pytorch.org/](https://pytorch.org/)
*   **Aprendizaje Continuo (Continual Learning):** [https://ruder.io/research/](https://ruder.io/research/)
*   **Estrategias de Aprendizaje Continuo:**
    *   **Finetuning:** Transferencia de aprendizaje, ajuste fino de un modelo pre-entrenado.
    *   **Replay:** Almacenamiento y reutilizaci贸n de ejemplos de tareas anteriores.
    *   **EWC (Elastic Weight Consolidation):** [https://arxiv.org/abs/1612.00796](https://arxiv.org/abs/1612.00796)
*   **Matriz de Confusi贸n:** [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)
